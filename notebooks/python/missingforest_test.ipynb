{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0a75c99-6a71-4b09-85ee-45d3d2b3fa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_mask(X, value_to_mask):\n",
    "    \"\"\"Compute the boolean mask X == missing_values.\"\"\"\n",
    "    if value_to_mask == \"NaN\" or np.isnan(value_to_mask):\n",
    "        return np.isnan(X)\n",
    "    else:\n",
    "        return X == value_to_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32344112-738a-4cd4-855a-305b246447e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"MissForest Imputer for Missing Data\"\"\"\n",
    "# Author: Ashim Bhattarai\n",
    "# License: GNU General Public License v3 (GPLv3)\n",
    "\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import mode\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils.validation import check_is_fitted, check_array\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "\n",
    "# from .pairwise_external import _get_mask\n",
    "\n",
    "__all__ = [\n",
    "    'MissForest',\n",
    "]\n",
    "\n",
    "\n",
    "class MissForest(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Missing value imputation using Random Forests.\n",
    "    MissForest imputes missing values using Random Forests in an iterative\n",
    "    fashion. By default, the imputer begins imputing missing values of the\n",
    "    column (which is expected to be a variable) with the smallest number of\n",
    "    missing values -- let's call this the candidate column.\n",
    "    The first step involves filling any missing values of the remaining,\n",
    "    non-candidate, columns with an initial guess, which is the column mean for\n",
    "    columns representing numerical variables and the column mode for columns\n",
    "    representing categorical variables. After that, the imputer fits a random\n",
    "    forest model with the candidate column as the outcome variable and the\n",
    "    remaining columns as the predictors over all rows where the candidate\n",
    "    column values are not missing.\n",
    "    After the fit, the missing rows of the candidate column are\n",
    "    imputed using the prediction from the fitted Random Forest. The\n",
    "    rows of the non-candidate columns act as the input data for the fitted\n",
    "    model.\n",
    "    Following this, the imputer moves on to the next candidate column with the\n",
    "    second smallest number of missing values from among the non-candidate\n",
    "    columns in the first round. The process repeats itself for each column\n",
    "    with a missing value, possibly over multiple iterations or epochs for\n",
    "    each column, until the stopping criterion is met.\n",
    "    The stopping criterion is governed by the \"difference\" between the imputed\n",
    "    arrays over successive iterations. For numerical variables (num_vars_),\n",
    "    the difference is defined as follows:\n",
    "     sum((X_new[:, num_vars_] - X_old[:, num_vars_]) ** 2) /\n",
    "     sum((X_new[:, num_vars_]) ** 2)\n",
    "    For categorical variables(cat_vars_), the difference is defined as follows:\n",
    "    sum(X_new[:, cat_vars_] != X_old[:, cat_vars_])) / n_cat_missing\n",
    "    where X_new is the newly imputed array, X_old is the array imputed in the\n",
    "    previous round, n_cat_missing is the total number of categorical\n",
    "    values that are missing, and the sum() is performed both across rows\n",
    "    and columns. Following [1], the stopping criterion is considered to have\n",
    "    been met when difference between X_new and X_old increases for the first\n",
    "    time for both types of variables (if available).\n",
    "    Parameters\n",
    "    ----------\n",
    "    NOTE: Most parameter definitions below are taken verbatim from the\n",
    "    Scikit-Learn documentation at [2] and [3].\n",
    "    max_iter : int, optional (default = 10)\n",
    "        The maximum iterations of the imputation process. Each column with a\n",
    "        missing value is imputed exactly once in a given iteration.\n",
    "    decreasing : boolean, optional (default = False)\n",
    "        If set to True, columns are sorted according to decreasing number of\n",
    "        missing values. In other words, imputation will move from imputing\n",
    "        columns with the largest number of missing values to columns with\n",
    "        fewest number of missing values.\n",
    "    missing_values : np.nan, integer, optional (default = np.nan)\n",
    "        The placeholder for the missing values. All occurrences of\n",
    "        `missing_values` will be imputed.\n",
    "    copy : boolean, optional (default = True)\n",
    "        If True, a copy of X will be created. If False, imputation will\n",
    "        be done in-place whenever possible.\n",
    "    criterion : tuple, optional (default = ('mse', 'gini'))\n",
    "        The function to measure the quality of a split.The first element of\n",
    "        the tuple is for the Random Forest Regressor (for imputing numerical\n",
    "        variables) while the second element is for the Random Forest\n",
    "        Classifier (for imputing categorical variables).\n",
    "    n_estimators : integer, optional (default=100)\n",
    "        The number of trees in the forest.\n",
    "    max_depth : integer or None, optional (default=None)\n",
    "        The maximum depth of the tree. If None, then nodes are expanded until\n",
    "        all leaves are pure or until all leaves contain less than\n",
    "        min_samples_split samples.\n",
    "    min_samples_split : int, float, optional (default=2)\n",
    "        The minimum number of samples required to split an internal node:\n",
    "        - If int, then consider `min_samples_split` as the minimum number.\n",
    "        - If float, then `min_samples_split` is a fraction and\n",
    "          `ceil(min_samples_split * n_samples)` are the minimum\n",
    "          number of samples for each split.\n",
    "    min_samples_leaf : int, float, optional (default=1)\n",
    "        The minimum number of samples required to be at a leaf node.\n",
    "        A split point at any depth will only be considered if it leaves at\n",
    "        least ``min_samples_leaf`` training samples in each of the left and\n",
    "        right branches.  This may have the effect of smoothing the model,\n",
    "        especially in regression.\n",
    "        - If int, then consider `min_samples_leaf` as the minimum number.\n",
    "        - If float, then `min_samples_leaf` is a fraction and\n",
    "          `ceil(min_samples_leaf * n_samples)` are the minimum\n",
    "          number of samples for each node.\n",
    "    min_weight_fraction_leaf : float, optional (default=0.)\n",
    "        The minimum weighted fraction of the sum total of weights (of all\n",
    "        the input samples) required to be at a leaf node. Samples have\n",
    "        equal weight when sample_weight is not provided.\n",
    "    max_features : int, float, string or None, optional (default=\"auto\")\n",
    "        The number of features to consider when looking for the best split:\n",
    "        - If int, then consider `max_features` features at each split.\n",
    "        - If float, then `max_features` is a fraction and\n",
    "          `int(max_features * n_features)` features are considered at each\n",
    "          split.\n",
    "        - If \"auto\", then `max_features=sqrt(n_features)`.\n",
    "        - If \"sqrt\", then `max_features=sqrt(n_features)` (same as \"auto\").\n",
    "        - If \"log2\", then `max_features=log2(n_features)`.\n",
    "        - If None, then `max_features=n_features`.\n",
    "        Note: the search for a split does not stop until at least one\n",
    "        valid partition of the node samples is found, even if it requires to\n",
    "        effectively inspect more than ``max_features`` features.\n",
    "    max_leaf_nodes : int or None, optional (default=None)\n",
    "        Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
    "        Best nodes are defined as relative reduction in impurity.\n",
    "        If None then unlimited number of leaf nodes.\n",
    "    min_impurity_decrease : float, optional (default=0.)\n",
    "        A node will be split if this split induces a decrease of the impurity\n",
    "        greater than or equal to this value.\n",
    "        The weighted impurity decrease equation is the following::\n",
    "            N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
    "                                - N_t_L / N_t * left_impurity)\n",
    "        where ``N`` is the total number of samples, ``N_t`` is the number of\n",
    "        samples at the current node, ``N_t_L`` is the number of samples in the\n",
    "        left child, and ``N_t_R`` is the number of samples in the right child.\n",
    "        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
    "        if ``sample_weight`` is passed.\n",
    "    bootstrap : boolean, optional (default=True)\n",
    "        Whether bootstrap samples are used when building trees.\n",
    "    oob_score : bool (default=False)\n",
    "        Whether to use out-of-bag samples to estimate\n",
    "        the generalization accuracy.\n",
    "    n_jobs : int or None, optional (default=None)\n",
    "        The number of jobs to run in parallel for both `fit` and `predict`.\n",
    "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "        for more details.\n",
    "    random_state : int, RandomState instance or None, optional (default=None)\n",
    "        If int, random_state is the seed used by the random number generator;\n",
    "        If RandomState instance, random_state is the random number generator;\n",
    "        If None, the random number generator is the RandomState instance used\n",
    "        by `np.random`.\n",
    "    verbose : int, optional (default=0)\n",
    "        Controls the verbosity when fitting and predicting.\n",
    "    warm_start : bool, optional (default=False)\n",
    "        When set to ``True``, reuse the solution of the previous call to fit\n",
    "        and add more estimators to the ensemble, otherwise, just fit a whole\n",
    "        new forest. See :term:`the Glossary <warm_start>`.\n",
    "    class_weight : dict, list of dicts, \"balanced\", \"balanced_subsample\" or \\\n",
    "    None, optional (default=None)\n",
    "        Weights associated with classes in the form ``{class_label: weight}``.\n",
    "        If not given, all classes are supposed to have weight one. For\n",
    "        multi-output problems, a list of dicts can be provided in the same\n",
    "        order as the columns of y.\n",
    "        Note that for multioutput (including multilabel) weights should be\n",
    "        defined for each class of every column in its own dict. For example,\n",
    "        for four-class multilabel classification weights should be\n",
    "        [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
    "        [{1:1}, {2:5}, {3:1}, {4:1}].\n",
    "        The \"balanced\" mode uses the values of y to automatically adjust\n",
    "        weights inversely proportional to class frequencies in the input data\n",
    "        as ``n_samples / (n_classes * np.bincount(y))``\n",
    "        The \"balanced_subsample\" mode is the same as \"balanced\" except that\n",
    "        weights are computed based on the bootstrap sample for every tree\n",
    "        grown.\n",
    "        For multi-output, the weights of each column of y will be multiplied.\n",
    "        Note that these weights will be multiplied with sample_weight (passed\n",
    "        through the fit method) if sample_weight is specified.\n",
    "        NOTE: This parameter is only applicable for Random Forest Classifier\n",
    "        objects (i.e., for categorical variables).\n",
    "    Attributes\n",
    "    ----------\n",
    "    statistics_ : Dictionary of length two\n",
    "        The first element is an array with the mean of each numerical feature\n",
    "        being imputed while the second element is an array of modes of\n",
    "        categorical features being imputed (if available, otherwise it\n",
    "        will be None).\n",
    "    References\n",
    "    ----------\n",
    "    * [1] Stekhoven, Daniel J., and Peter Bühlmann. \"MissForest—non-parametric\n",
    "      missing value imputation for mixed-type data.\" Bioinformatics 28.1\n",
    "      (2011): 112-118.\n",
    "    * [2] https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.\n",
    "      RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor\n",
    "    * [3] https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.\n",
    "      RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from missingpy import MissForest\n",
    "    >>> nan = float(\"NaN\")\n",
    "    >>> X = [[1, 2, nan], [3, 4, 3], [nan, 6, 5], [8, 8, 7]]\n",
    "    >>> imputer = MissForest(random_state=1337)\n",
    "    >>> imputer.fit_transform(X)\n",
    "    Iteration: 0\n",
    "    Iteration: 1\n",
    "    Iteration: 2\n",
    "    array([[1.  , 2. , 3.92 ],\n",
    "           [3.  , 4. , 3. ],\n",
    "           [2.71, 6. , 5. ],\n",
    "           [8.  , 8. , 7. ]])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_iter=10, decreasing=False, missing_values=np.nan,\n",
    "                 copy=True, n_estimators=100, criterion=('mse', 'gini'),\n",
    "                 max_depth=None, min_samples_split=2, min_samples_leaf=1,\n",
    "                 min_weight_fraction_leaf=0.0, max_features='auto',\n",
    "                 max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "                 bootstrap=True, oob_score=False, n_jobs=-1, random_state=None,\n",
    "                 verbose=0, warm_start=False, class_weight=None):\n",
    "\n",
    "        self.max_iter = max_iter\n",
    "        self.decreasing = decreasing\n",
    "        self.missing_values = missing_values\n",
    "        self.copy = copy\n",
    "        self.n_estimators = n_estimators\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.min_weight_fraction_leaf = min_weight_fraction_leaf\n",
    "        self.max_features = max_features\n",
    "        self.max_leaf_nodes = max_leaf_nodes\n",
    "        self.min_impurity_decrease = min_impurity_decrease\n",
    "        self.bootstrap = bootstrap\n",
    "        self.oob_score = oob_score\n",
    "        self.n_jobs = n_jobs\n",
    "        self.random_state = random_state\n",
    "        self.verbose = verbose\n",
    "        self.warm_start = warm_start\n",
    "        self.class_weight = class_weight\n",
    "\n",
    "    def _miss_forest(self, Ximp, mask):\n",
    "        \"\"\"The missForest algorithm\"\"\"\n",
    "\n",
    "        # Count missing per column\n",
    "        col_missing_count = mask.sum(axis=0)\n",
    "\n",
    "        # Get col and row indices for missing\n",
    "        missing_rows, missing_cols = np.where(mask)\n",
    "\n",
    "        if self.num_vars_ is not None:\n",
    "            # Only keep indices for numerical vars\n",
    "            keep_idx_num = np.in1d(missing_cols, self.num_vars_)\n",
    "            missing_num_rows = missing_rows[keep_idx_num]\n",
    "            missing_num_cols = missing_cols[keep_idx_num]\n",
    "\n",
    "            # Make initial guess for missing values\n",
    "            col_means = np.full(Ximp.shape[1], fill_value=np.nan)\n",
    "            col_means[self.num_vars_] = self.statistics_.get('col_means')\n",
    "            Ximp[missing_num_rows, missing_num_cols] = np.take(\n",
    "                col_means, missing_num_cols)\n",
    "\n",
    "            # Reg criterion\n",
    "            reg_criterion = self.criterion if type(self.criterion) == str \\\n",
    "                else self.criterion[0]\n",
    "\n",
    "            # Instantiate regression model\n",
    "            rf_regressor = RandomForestRegressor(\n",
    "                n_estimators=self.n_estimators,\n",
    "                criterion=reg_criterion,\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_split=self.min_samples_split,\n",
    "                min_samples_leaf=self.min_samples_leaf,\n",
    "                min_weight_fraction_leaf=self.min_weight_fraction_leaf,\n",
    "                max_features=self.max_features,\n",
    "                max_leaf_nodes=self.max_leaf_nodes,\n",
    "                min_impurity_decrease=self.min_impurity_decrease,\n",
    "                bootstrap=self.bootstrap,\n",
    "                oob_score=self.oob_score,\n",
    "                n_jobs=self.n_jobs,\n",
    "                random_state=self.random_state,\n",
    "                verbose=self.verbose,\n",
    "                warm_start=self.warm_start)\n",
    "\n",
    "        # If needed, repeat for categorical variables\n",
    "        if self.cat_vars_ is not None:\n",
    "            # Calculate total number of missing categorical values (used later)\n",
    "            n_catmissing = np.sum(mask[:, self.cat_vars_])\n",
    "\n",
    "            # Only keep indices for categorical vars\n",
    "            keep_idx_cat = np.in1d(missing_cols, self.cat_vars_)\n",
    "            missing_cat_rows = missing_rows[keep_idx_cat]\n",
    "            missing_cat_cols = missing_cols[keep_idx_cat]\n",
    "\n",
    "            # Make initial guess for missing values\n",
    "            col_modes = np.full(Ximp.shape[1], fill_value=np.nan)\n",
    "            col_modes[self.cat_vars_] = self.statistics_.get('col_modes')\n",
    "            Ximp[missing_cat_rows, missing_cat_cols] = np.take(col_modes, missing_cat_cols)\n",
    "\n",
    "            # Classfication criterion\n",
    "            clf_criterion = self.criterion if type(self.criterion) == str \\\n",
    "                else self.criterion[1]\n",
    "\n",
    "            # Instantiate classification model\n",
    "            rf_classifier = RandomForestClassifier(\n",
    "                n_estimators=self.n_estimators,\n",
    "                criterion=clf_criterion,\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_split=self.min_samples_split,\n",
    "                min_samples_leaf=self.min_samples_leaf,\n",
    "                min_weight_fraction_leaf=self.min_weight_fraction_leaf,\n",
    "                max_features=self.max_features,\n",
    "                max_leaf_nodes=self.max_leaf_nodes,\n",
    "                min_impurity_decrease=self.min_impurity_decrease,\n",
    "                bootstrap=self.bootstrap,\n",
    "                oob_score=self.oob_score,\n",
    "                n_jobs=self.n_jobs,\n",
    "                random_state=self.random_state,\n",
    "                verbose=self.verbose,\n",
    "                warm_start=self.warm_start,\n",
    "                class_weight=self.class_weight)\n",
    "\n",
    "        # 2. misscount_idx: sorted indices of cols in X based on missing count\n",
    "        misscount_idx = np.argsort(col_missing_count)\n",
    "        # Reverse order if decreasing is set to True\n",
    "        if self.decreasing is True:\n",
    "            misscount_idx = misscount_idx[::-1]\n",
    "\n",
    "        # 3. While new_gammas < old_gammas & self.iter_count_ < max_iter loop:\n",
    "        self.iter_count_ = 0\n",
    "        gamma_new = 0\n",
    "        gamma_old = np.inf\n",
    "        gamma_newcat = 0\n",
    "        gamma_oldcat = np.inf\n",
    "        col_index = np.arange(Ximp.shape[1])\n",
    "\n",
    "        while (\n",
    "                gamma_new < gamma_old or gamma_newcat < gamma_oldcat) and \\\n",
    "                self.iter_count_ < self.max_iter:\n",
    "\n",
    "            # 4. store previously imputed matrix\n",
    "            Ximp_old = np.copy(Ximp)\n",
    "            if self.iter_count_ != 0:\n",
    "                gamma_old = gamma_new\n",
    "                gamma_oldcat = gamma_newcat\n",
    "            # 5. loop\n",
    "            for s in misscount_idx:\n",
    "                # Column indices other than the one being imputed\n",
    "                s_prime = np.delete(col_index, s)\n",
    "\n",
    "                # Get indices of rows where 's' is observed and missing\n",
    "                obs_rows = np.where(~mask[:, s])[0]\n",
    "                mis_rows = np.where(mask[:, s])[0]\n",
    "\n",
    "                # If no missing, then skip\n",
    "                if len(mis_rows) == 0:\n",
    "                    continue\n",
    "\n",
    "                # Get observed values of 's'\n",
    "                yobs = Ximp[obs_rows, s]\n",
    "\n",
    "                # Get 'X' for both observed and missing 's' column\n",
    "                xobs = Ximp[np.ix_(obs_rows, s_prime)]\n",
    "                xmis = Ximp[np.ix_(mis_rows, s_prime)]\n",
    "\n",
    "                # 6. Fit a random forest over observed and predict the missing\n",
    "                if self.cat_vars_ is not None and s in self.cat_vars_:\n",
    "                    rf_classifier.fit(X=xobs, y=yobs)\n",
    "                    # 7. predict ymis(s) using xmis(x)\n",
    "                    ymis = rf_classifier.predict(xmis)\n",
    "                    # 8. update imputed matrix using predicted matrix ymis(s)\n",
    "                    Ximp[mis_rows, s] = ymis\n",
    "                else:\n",
    "                    rf_regressor.fit(X=xobs, y=yobs)\n",
    "                    # 7. predict ymis(s) using xmis(x)\n",
    "                    ymis = rf_regressor.predict(xmis)\n",
    "                    # 8. update imputed matrix using predicted matrix ymis(s)\n",
    "                    Ximp[mis_rows, s] = ymis\n",
    "\n",
    "            # 9. Update gamma (stopping criterion)\n",
    "            if self.cat_vars_ is not None:\n",
    "                gamma_newcat = np.sum(\n",
    "                    (Ximp[:, self.cat_vars_] != Ximp_old[:, self.cat_vars_])) / n_catmissing\n",
    "            if self.num_vars_ is not None:\n",
    "                gamma_new = np.sum((Ximp[:, self.num_vars_] - Ximp_old[:, self.num_vars_]) ** 2) / np.sum((Ximp[:, self.num_vars_]) ** 2)\n",
    "\n",
    "            print(\"Iteration:\", self.iter_count_)\n",
    "            self.iter_count_ += 1\n",
    "\n",
    "        return Ximp_old\n",
    "\n",
    "    def fit(self, X, y=None, cat_vars=None):\n",
    "        \"\"\"Fit the imputer on X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like}, shape (n_samples, n_features)\n",
    "            Input data, where ``n_samples`` is the number of samples and\n",
    "            ``n_features`` is the number of features.\n",
    "        cat_vars : int or array of ints, optional (default = None)\n",
    "            An int or an array containing column indices of categorical\n",
    "            variable(s)/feature(s) present in the dataset X.\n",
    "            ``None`` if there are no categorical variables in the dataset.\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns self.\n",
    "        \"\"\"\n",
    "\n",
    "        # Check data integrity and calling arguments\n",
    "        force_all_finite = False if self.missing_values in [\"NaN\",\n",
    "                                                            np.nan] else True\n",
    "\n",
    "        X = check_array(X, accept_sparse=False, dtype=np.float64,\n",
    "                        force_all_finite=force_all_finite, copy=self.copy)\n",
    "\n",
    "        # Check for +/- inf\n",
    "        if np.any(np.isinf(X)):\n",
    "            raise ValueError(\"+/- inf values are not supported.\")\n",
    "\n",
    "        # Check if any column has all missing\n",
    "        mask = _get_mask(X, self.missing_values)\n",
    "        if np.any(mask.sum(axis=0) >= (X.shape[0])):\n",
    "            raise ValueError(\"One or more columns have all rows missing.\")\n",
    "\n",
    "        # Check cat_vars type and convert if necessary\n",
    "        if cat_vars is not None:\n",
    "            if type(cat_vars) == int:\n",
    "                cat_vars = [cat_vars]\n",
    "            elif type(cat_vars) == list or type(cat_vars) == np.ndarray:\n",
    "                if np.array(cat_vars).dtype != int:\n",
    "                    raise ValueError(\n",
    "                        \"cat_vars needs to be either an int or an array \"\n",
    "                        \"of ints.\")\n",
    "            else:\n",
    "                raise ValueError(\"cat_vars needs to be either an int or an array \"\n",
    "                                 \"of ints.\")\n",
    "\n",
    "        # Identify numerical variables\n",
    "        num_vars = np.setdiff1d(np.arange(X.shape[1]), cat_vars)\n",
    "        num_vars = num_vars if len(num_vars) > 0 else None\n",
    "\n",
    "        # First replace missing values with NaN if it is something else\n",
    "        if self.missing_values not in ['NaN', np.nan]:\n",
    "            X[np.where(X == self.missing_values)] = np.nan\n",
    "\n",
    "        # Now, make initial guess for missing values\n",
    "        col_means = np.nanmean(X[:, num_vars], axis=0) if num_vars is not None else None\n",
    "        col_modes = mode(\n",
    "            X[:, cat_vars], axis=0, nan_policy='omit')[0] if cat_vars is not \\\n",
    "                                                           None else None\n",
    "\n",
    "        self.cat_vars_ = cat_vars\n",
    "        self.num_vars_ = num_vars\n",
    "        self.statistics_ = {\"col_means\": col_means, \"col_modes\": col_modes}\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Impute all missing values in X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like}, shape = [n_samples, n_features]\n",
    "            The input data to complete.\n",
    "        Returns\n",
    "        -------\n",
    "        X : {array-like}, shape = [n_samples, n_features]\n",
    "            The imputed dataset.\n",
    "        \"\"\"\n",
    "        # Confirm whether fit() has been called\n",
    "        check_is_fitted(self, [\"cat_vars_\", \"num_vars_\", \"statistics_\"])\n",
    "\n",
    "        # Check data integrity\n",
    "        force_all_finite = False if self.missing_values in [\"NaN\",\n",
    "                                                            np.nan] else True\n",
    "        X = check_array(X, accept_sparse=False, dtype=np.float64,\n",
    "                        force_all_finite=force_all_finite, copy=self.copy)\n",
    "\n",
    "        # Check for +/- inf\n",
    "        if np.any(np.isinf(X)):\n",
    "            raise ValueError(\"+/- inf values are not supported.\")\n",
    "\n",
    "        # Check if any column has all missing\n",
    "        mask = _get_mask(X, self.missing_values)\n",
    "        if np.any(mask.sum(axis=0) >= (X.shape[0])):\n",
    "            raise ValueError(\"One or more columns have all rows missing.\")\n",
    "\n",
    "        # Get fitted X col count and ensure correct dimension\n",
    "        n_cols_fit_X = (0 if self.num_vars_ is None else len(self.num_vars_)) \\\n",
    "            + (0 if self.cat_vars_ is None else len(self.cat_vars_))\n",
    "        _, n_cols_X = X.shape\n",
    "\n",
    "        if n_cols_X != n_cols_fit_X:\n",
    "            raise ValueError(\"Incompatible dimension between the fitted \"\n",
    "                             \"dataset and the one to be transformed.\")\n",
    "\n",
    "        # Check if anything is actually missing and if not return original X                             \n",
    "        mask = _get_mask(X, self.missing_values)\n",
    "        if not mask.sum() > 0:\n",
    "            warnings.warn(\"No missing value located; returning original \"\n",
    "                          \"dataset.\")\n",
    "            return X\n",
    "\n",
    "        # row_total_missing = mask.sum(axis=1)\n",
    "        # if not np.any(row_total_missing):\n",
    "        #     return X\n",
    "\n",
    "        # Call missForest function to impute missing\n",
    "        X = self._miss_forest(X, mask)\n",
    "\n",
    "        # Return imputed dataset\n",
    "        return X\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        \"\"\"Fit MissForest and impute all missing values in X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like}, shape (n_samples, n_features)\n",
    "            Input data, where ``n_samples`` is the number of samples and\n",
    "            ``n_features`` is the number of features.\n",
    "        Returns\n",
    "        -------\n",
    "        X : {array-like}, shape (n_samples, n_features)\n",
    "            Returns imputed dataset.\n",
    "        \"\"\"\n",
    "        return self.fit(X, **fit_params).transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d27c024-62da-4432-8271-280cb1648438",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/superbugai-data/yash/chapter_1/workspace/EHRQC/notebooks/.venv/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:387: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Iteration: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/superbugai-data/yash/chapter_1/workspace/EHRQC/notebooks/.venv/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:387: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
      "  warn(\n",
      "/superbugai-data/yash/chapter_1/workspace/EHRQC/notebooks/.venv/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:387: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 2.  ,  2.  ,  2.  ,  1.  ],\n",
       "       [ 3.  ,  2.  ,  3.  ,  2.  ],\n",
       "       [ 4.31,  4.  ,  5.  ,  5.  ],\n",
       "       [ 6.  ,  7.  ,  6.  ,  7.  ],\n",
       "       [ 9.  ,  9.  ,  8.  ,  8.  ],\n",
       "       [16.  , 15.  , 18.  , 19.  ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    X = np.array([\n",
    "        [1,      0,      0,      1],\n",
    "        [2,      1,      2,      2],\n",
    "        [3,      2,      3,      2],\n",
    "        [np.nan, 4,      5,      5],\n",
    "        [6,      7,      6,      7],\n",
    "        [8,      8,      8,      8],\n",
    "        [16,     15,     18,    19],\n",
    "    ])\n",
    "    statistics_mean = np.nanmean(X, axis=0)\n",
    "\n",
    "    Y = np.array([\n",
    "        [0,      0,      0,      0],\n",
    "        [2,      2,      2,      1],\n",
    "        [3,      2,      3,      2],\n",
    "        [np.nan, 4,      5,      5],\n",
    "        [6,      7,      6,      7],\n",
    "        [9,      9,      8,      8],\n",
    "        [16,     15,     18,    19],\n",
    "    ])\n",
    "\n",
    "    imputer = MissForest()\n",
    "    imputer.fit(X).transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a58146-9ce9-488e-8411-1bae8b39ef8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
